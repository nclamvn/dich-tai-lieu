Title: On the Convergence Properties of Gradient Descent in Non-Convex Optimization

Abstract

We analyze the convergence behavior of gradient descent algorithms in non-convex optimization landscapes. Our main result establishes that under mild smoothness assumptions, gradient descent with appropriate step size converges to a stationary point at rate $O(1/\sqrt{T})$.

1. Introduction

Consider the unconstrained optimization problem:

$\min_{x \in \mathbb{R}^n} f(x)$

where $f: \mathbb{R}^n \to \mathbb{R}$ is a continuously differentiable function. The gradient descent algorithm updates the iterate according to:

$x_{t+1} = x_t - \eta \nabla f(x_t)$

where $\eta > 0$ is the step size (learning rate).

2. Theoretical Framework

Definition 2.1 (L-Smoothness): A function $f$ is L-smooth if for all $x, y \in \mathbb{R}^n$:

$\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|$

Lemma 2.2: For L-smooth functions, the following descent inequality holds:

$f(x_{t+1}) \leq f(x_t) - \eta \|\nabla f(x_t)\|^2 + \frac{L\eta^2}{2} \|\nabla f(x_t)\|^2$

Proof: By L-smoothness and Taylor expansion:

$f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2}\|y-x\|^2$

Setting $y = x_{t+1} = x_t - \eta \nabla f(x_t)$, we obtain the result. $\square$

3. Main Results

Theorem 3.1 (Convergence Rate): Let $f$ be L-smooth and bounded below by $f^*$. For step size $\eta = 1/L$, gradient descent satisfies:

$\min_{t=0,\ldots,T-1} \|\nabla f(x_t)\|^2 \leq \frac{2L(f(x_0) - f^*)}{T}$

Corollary 3.2: The algorithm finds an $\epsilon$-stationary point (where $\|\nabla f(x)\| \leq \epsilon$) in at most $T = O(1/\epsilon^2)$ iterations.

4. Numerical Experiments

We tested our theoretical bounds on the Rosenbrock function:

$f(x,y) = (1-x)^2 + 100(y-x^2)^2$

Table 1: Convergence results
| Iterations | Gradient Norm | Function Value |
|------------|---------------|----------------|
| 100        | 2.34e-2       | 1.23e-3        |
| 1000       | 7.89e-4       | 4.56e-6        |
| 10000      | 2.45e-5       | 1.23e-9        |

5. Conclusion

We have established convergence guarantees for gradient descent in non-convex settings. Future work will extend these results to stochastic variants and adaptive step sizes.

References

[1] Nesterov, Y. (2004). Introductory Lectures on Convex Optimization.
[2] Bottou, L. et al. (2018). Optimization Methods for Large-Scale Machine Learning.
